{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Backward.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"f9m4QEP_P35X","colab_type":"code","outputId":"e37e46cf-6f07-40f2-a683-ef3f600e8fe5","executionInfo":{"status":"ok","timestamp":1570074335778,"user_tz":180,"elapsed":76354,"user":{"displayName":"Paulo Augusto Alves Luz Viana","photoUrl":"","userId":"17456686599891736397"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","import os\n","CODE_PATH='/content/drive/My Drive/TCC/Codes/Codigos Atualizados/'\n","DATA_PATH = '/content/drive/My Drive/TCC/EMG_Database/'\n","drive.mount('/content/drive', force_remount=True)\n","if not os.getcwd() == CODE_PATH:\n","  os.chdir(CODE_PATH)\n","# google.colab.drive module has a recently-added flush_and_unmount()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KA8-YaFBQBxE","colab_type":"code","colab":{}},"source":["from numpy.random import seed\n","from tensorflow import set_random_seed\n","# from algorithms import KNN, LogisticRegression, Neural_Network"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ov6cPjCGxfvd","colab_type":"code","outputId":"55d171e7-7a80-47d7-f401-d1b515896129","executionInfo":{"status":"ok","timestamp":1570074340920,"user_tz":180,"elapsed":11103,"user":{"displayName":"Paulo Augusto Alves Luz Viana","photoUrl":"","userId":"17456686599891736397"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Sep  4 17:57:26 2019\n","\n","@author: Paulo\n","\"\"\"\n","\n","import numpy as np\n","import os\n","\n","from keras.utils import Progbar\n","from keras.utils import to_categorical\n","\n","from keras.optimizers import Adam\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.regularizers import l2, l1_l2\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TerminateOnNaN, EarlyStopping\n","from keras.constraints import MinMaxNorm\n","import keras.backend as K\n","from sklearn.utils import class_weight\n","\n","from os.path import join\n","import warnings\n","\n","warnings.simplefilter(\"ignore\")\n","\n","\n","from sklearn.preprocessing import PowerTransformer, QuantileTransformer, RobustScaler\n","from cleaners import DataClipper, FeatureSelector\n","from ModelCollection import DataSelection\n","import joblib\n","\n","from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report, cohen_kappa_score, roc_auc_score, precision_score, recall_score\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","\n","from numpy.random import seed\n","from tensorflow import set_random_seed\n","\n","def dist(a, b):\n","  res = np.sqrt(((a-b)**2).sum(axis=1))\n","  return res\n","\n","def dist_balanced(distance):\n","    return 1/(distance + 1e-3)\n","\n","def abs_KL_div(y_true, y_pred):\n","    y_true = K.clip(y_true, K.epsilon(), None)\n","    y_pred = K.clip(y_pred, K.epsilon(), None)\n","    return K.sum( K.abs( (y_true- y_pred) * (K.log(y_true / y_pred))), axis=-1)\n","\n","TRAIN_IDS = np.array([39, 34, 19, 30, 21, 40,  6, 18, 12, 31,  4, 27, 20, 15,  7, 22,  8, 35, 10, 37, 14, 23, 25,  2, 17, 38, 26, 16])\n","VALID_IDS = np.array([ 9, 29,  5,  1, 13, 36])\n","TEST_IDS = np.array([32, 33,  3, 11, 24, 28])\n","\n","vote_funcs = {\"dist\": dist_balanced,\n","              \"uniform\": lambda x: 1}\n","\n","class KNN():\n","\n","    def __init__(self, n_neighbors=370, mode=\"dist\", verbose=False):\n","        self.k = n_neighbors\n","        self.vote_func = vote_funcs[mode]\n","        self.fitted = False\n","        self.verbose = verbose\n","        \n","    def fit(self, X, y):\n","        self.X = X\n","        self.y = y\n","        if np.ndim(y)<2:\n","            self.encoder = LabelEncoder()\n","            self.y = to_categorical(self.encoder.fit_transform(y))\n","        self.fitted = True\n","        return self\n","      \n","    def predict(self, X):\n","        bar = Progbar(len(X))\n","        pred_proba = []\n","        for i in range(len(X)):\n","          if self.verbose: bar.add(1)\n","          ordered_dists = dist(self.X, X[i])\n","          ordered_dists = sorted(zip(ordered_dists, range(len(ordered_dists))), key=lambda x:x[0])\n","          k_indexes = [x[1] for x in ordered_dists[:self.k]]\n","          votes = np.zeros(self.y.shape[1])\n","          for p, x in enumerate(k_indexes):\n","              votes[self.y[x].argmax()] += self.vote_func(ordered_dists[p][0])\n","          pred_proba.append(votes)\n","        return to_categorical(np.array(pred_proba).argmax(axis=1))\n","\n","class LogisticRegression():\n","  \n","    def __init__(self, inputs, outputs, save_path=\".\", reg_l1=0, reg_l2=0, lr=1e-3):\n","        self.inputs = inputs\n","        self.outputs = outputs\n","        self.reg_l1 = reg_l1\n","        self.reg_l2 = reg_l2\n","        self.lr = lr\n","        self.save_path = save_path\n","        self.seed = 666\n","        self.create_model()\n","        self.create_callbacks()\n","        self.fitted = False\n","        \n","    def create_callbacks(self, patience_rlop=10, patience_es=20):\n","\n","        cpkt_path = os.path.join(self.save_path, \"model.cpkt\")\n","        log_path = os.path.join(self.save_path, \"log.csv\")\n","\n","        self.callbacks = [ReduceLROnPlateau(monitor=\"val_loss\", patience=patience_rlop, factor=0.1, min_delta=0.0001, verbose=0),\n","                          EarlyStopping(monitor=\"val_loss\", patience=patience_es, min_delta=0.0001, restore_best_weights=True, verbose=0),\n","                          TerminateOnNaN()]\n","        if not self.save_path is None:\n","          self.callbacks += [ModelCheckpoint(cpkt_path, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False),\n","                             CSVLogger(log_path, separator=',', append=False)]\n","\n","    def create_model(self):\n","        set_random_seed(666)\n","        seed(self.seed)\n","        model = Sequential()\n","        model.add(Dense(input_shape=(self.inputs,),\n","                        units=self.outputs,\n","                        activation=\"softmax\",\n","                        kernel_initializer=\"glorot_normal\",\n","                        kernel_regularizer=l1_l2(l1=self.reg_l1, l2=self.reg_l2),\n","                        bias_regularizer=l1_l2(l1=self.reg_l1, l2=self.reg_l2)))\n","        loss = \"categorical_crossentropy\" if self.outputs>1 else \"binary_crossentropy\"\n","        model.compile(loss=loss, metrics=[\"acc\"], optimizer=Adam(lr=self.lr))\n","        self.model = model\n","\n","    def fit(self, *args, **kwargs):\n","        self.fitted = False\n","        set_random_seed(self.seed)\n","        seed(self.seed)\n","        kwargs.update(callbacks=self.callbacks)\n","        self.model.fit(*args, **kwargs)\n","        self.fitted = True\n","        if not self.save_path is None:\n","          self.model.save(os.path.join(self.save_path, \"model.K\"))\n","        \n","    def predict(self, x):\n","        return self.model.predict(x)\n","\n","    def save(self, path):\n","        self.model.save(path)\n","\n","\n","class Neural_Network():\n","\n","    def __init__(self, input_dim, output_dim, lr=1e-5, activation=\"selu\", n_layers=3,\n","                 max_norm=.5, dropouts=[.3, .2, 0], units=[200, 150,100],\n","                 rk =[1e-4]*3, rb=[1e-3]*3, save_path=\".\"):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.model_kwargs = dict(lr=lr, activation=\"selu\", n_layers=n_layers,\n","                                 max_norm=max_norm, dropouts=dropouts,\n","                                 units=units, rk=rk,\n","                                 rb=rb)\n","        self.callbacks = []\n","        self.save_path = save_path\n","        self.seed = 666\n","        self.create_model()\n","        self.create_callbacks()\n","        self.fitted = False\n","        \n","    def create_callbacks(self, patience_rlop=10, patience_es=30):\n","\n","        cpkt_path = os.path.join(self.save_path, \"model.cpkt\")\n","        log_path = os.path.join(self.save_path, \"log.csv\")\n","\n","        self.callbacks = [ReduceLROnPlateau(monitor=\"val_loss\", patience=patience_rlop, factor=0.1, min_delta=0.0001, verbose=0),\n","                          EarlyStopping(monitor=\"val_loss\", patience=patience_es, min_delta=0.0001, restore_best_weights=True, verbose=0),\n","                          TerminateOnNaN()]\n","        if not self.save_path is None:\n","          self.callbacks += [ModelCheckpoint(cpkt_path, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False),\n","                             CSVLogger(log_path, separator=',', append=False)]\n","\n","    def create_model(self, clear_session=False):\n","        if clear_session: K.clear_session()\n","        set_random_seed(666)\n","        seed(self.seed)\n","        model = Sequential()\n","        activation = self.model_kwargs[\"activation\"]\n","        max_norm = self.model_kwargs[\"max_norm\"]\n","        n_layers = self.model_kwargs[\"n_layers\"]\n","        d = self.model_kwargs[\"dropouts\"]\n","        units = self.model_kwargs[\"units\"]\n","        rk = self.model_kwargs[\"rk\"]\n","        rb = self.model_kwargs[\"rb\"]\n","        lr = self.model_kwargs[\"lr\"]\n","\n","        model.add(Dense(input_shape=(self.input_dim,),\n","                          units=units[0],\n","                          kernel_initializer=\"glorot_normal\",\n","                          kernel_regularizer=l2(rk[0]),\n","                          kernel_constraint=MinMaxNorm(0, max_norm),\n","                          bias_regularizer=l2(rb[0]),\n","                          activation=activation))\n","        model.add(Dropout(d[0]))\n","\n","        for i in range(n_layers-1):\n","          model.add(Dense(units=units[i+1],\n","                          kernel_initializer=\"glorot_normal\",\n","                          kernel_regularizer=l2(rk[i+1]),\n","                          bias_regularizer=l2(rb[i+1]),\n","                          kernel_constraint=MinMaxNorm(0, max_norm),\n","                          activation=activation))\n","          model.add(Dropout(d[i+1]))\n","\n","        model.add(Dense(units=self.output_dim,\n","                      kernel_initializer=\"glorot_normal\",\n","                      kernel_regularizer=l2(rk[-1]),\n","                      bias_regularizer=l2(rb[-1]),\n","                      kernel_constraint=MinMaxNorm(0, max_norm),\n","                      activation=\"softmax\"))\n","        optimizer = Adam(lr=lr, clipnorm=.2)\n","        model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"acc\"])\n","        self.model = model\n","\n","    def fit(self, *args, **kwargs):\n","        self.fitted = False\n","        set_random_seed(self.seed)\n","        seed(self.seed)\n","        kwargs.update(callbacks=self.callbacks)\n","        self.model.fit(*args, **kwargs)\n","        self.fitted = True\n","        if not self.save_path is None:\n","          self.model.save(os.path.join(self.save_path, \"model.K\"))\n","\n","    def predict(self, *args, **kwargs):\n","        return self.model.predict(*args, **kwargs)\n","\n","    def save(self, path):\n","        self.model.save(path)\n","\n","def make_metrics(label, prediction, ids=None, folder=\".\", mode=\"w\", identifier=\"\"):\n","\n","    bas, f1, k, p, r, auc = [], [], [], [], [], []\n","    for sub in np.unique(ids):\n","        ctt = ids==sub\n","        data = (label[ctt].argmax(1), prediction[ctt].argmax(1))\n","        bas.append(balanced_accuracy_score(*data))\n","        f1.append(f1_score(*data, average=\"weighted\"))\n","        k.append(cohen_kappa_score(*data))\n","        try:\n","          auc.append(roc_auc_score(label[ctt], prediction[ctt], average=\"weighted\"))\n","        except:\n","          print(f\"AUC error at subject {sub}\")\n","          auc.append(0)\n","        p.append(precision_score(*data, average=\"weighted\"))\n","        r.append(recall_score(*data, average=\"weighted\"))\n","    \n","    mean = np.mean\n","    std = np.std\n","    \n","    with open(os.path.join(folder, identifier+\"results_detailed.txt\"), mode) as file:\n","      print(\"Balanced Accuracy, F1 Score, Cohen Kappa, AUC, Precision, Recall\", file=file)\n","      for i in range(len(bas)):\n","        print(f\"{bas[i]}, {f1[i]}, {k[i]}, {auc[i]}, {p[i]}, {r[i]}\",file=file)\n","    \n","    with open(os.path.join(folder, identifier+\"results.txt\"), mode) as file:\n","        print(\"N. examples:\", prediction.shape[0], file=file)\n","        print(\"Balanced Accuracy:\", min(bas), max(bas), mean(bas), std(bas), file=file)\n","        print(\"F1 Score:\", min(f1), max(f1), mean(f1), std(f1), file=file)\n","        print(\"Cohen Kappa:\", min(k), max(k), mean(k), std(k), file=file)\n","        print(\"AUC:\", min(auc), max(auc), mean(auc), std(auc), file=file)\n","        print(\"Precision:\", min(p), max(p), mean(p), std(p), file=file)\n","        print(\"Recall:\", min(r), max(r), mean(r), std(r), file=file)\n","    with open(os.path.join(folder, identifier+\"report.txt\"), mode) as file:\n","        print(classification_report(label.argmax(1), prediction.argmax(1)), file=file)\n","\n","def create_folds(features, labels, ids, mixed_subjects=False,\n","                 save_path=\".\", feature_cols=None, movs=range(1, 50),\n","                 train_ids=TRAIN_IDS, valid_ids=VALID_IDS,\n","                 test_ids=TEST_IDS):\n","\n","    x_train = []\n","    x_valid = []\n","    x_test = []\n","    y_train = []\n","    y_valid = []\n","    y_test = []\n","\n","    ids_train, ids_valid, ids_test = [], [], []\n","\n","    selector = DataSelection(ids)\n","    ctt = selector.select(labels, subs=\"all\", movs=movs)\n","    \n","    print(sum(ctt), \"out of\", features.shape[0], \"selected\")\n","\n","    print(\"Selecting movements\")\n","\n","    features_ = features[ctt]\n","    features_ = features_[:, feature_cols]\n","    ids_ = ids[ctt]\n","    labels_ = labels[ctt]\n","\n","    print(\"Folding subjects\")\n","    if not mixed_subjects:\n","\n","      ctt_train = [x in train_ids for x in ids_]\n","      ctt_valid= [x in valid_ids for x in ids_]\n","      ctt_test = [x in test_ids for x in ids_]\n","      \n","      x_train = features_[ctt_train]\n","      x_valid = features_[ctt_valid]\n","      x_test = features_[ctt_test]\n","\n","      y_train = labels_[ctt_train]\n","      y_valid = labels_[ctt_valid]\n","      y_test = labels_[ctt_test]\n","\n","      ids_train = ids_[ctt_train]\n","      ids_valid = ids_[ctt_valid]\n","      ids_test = ids_[ctt_test]\n","\n","    else:\n","      x_train, x_test, y_train, y_test, ids_train, ids_test = train_test_split( features_, labels_, ids_, test_size=0.3, random_state=42)\n","      x_test, x_valid, y_test, y_valid, ids_test, ids_valid = train_test_split( x_test, y_test, ids_test, test_size=0.5, random_state=42)\n","                                                                               \n","    print(\"TRAIN #\", x_train.shape[0])\n","    print(\"VALID #\", x_valid.shape[0])\n","    print(\"TEST #\", x_test.shape[0])\n","\n","    encoder = LabelEncoder()\n","    onehot =  OneHotEncoder()\n","    y_train = np.array(onehot.fit_transform(encoder.fit_transform(y_train).reshape(-1, 1)).todense())\n","    y_valid = np.array(onehot.transform(encoder.transform(y_valid).reshape(-1, 1)).todense())\n","    y_test = np.array(onehot.transform(encoder.transform(y_test).reshape(-1, 1)).todense())\n","\n","    print(\"Scaling\")\n","\n","    scaler1 = RobustScaler()\n","    x_train = scaler1.fit_transform(x_train)\n","    x_valid = scaler1.transform(x_valid)\n","    x_test = scaler1.transform(x_test)\n","\n","    scaler2 = PowerTransformer()\n","    x_train = scaler2.fit_transform(x_train)\n","    x_valid = scaler2.transform(x_valid)\n","    x_test = scaler2.transform(x_test)\n","\n","#     if not isinstance(y_train, np.ndarray): y_train = y_train.todense()\n","#     if not isinstance(y_valid, np.ndarray): y_valid = y_train.todense()\n","#     if not isinstance(y_test, np.ndarray): y_test = y_test.todense()\n","    \n","    return (x_train, y_train, ids_train), \\\n","            (x_valid, y_valid, ids_valid), \\\n","            (x_test, y_test, ids_test), \\\n","            (onehot, encoder, scaler1, scaler2)\n","\n","def save_transformers(transformers, names=[], path=\".\"):\n","    for t, name in zip(transformers, names):\n","        name = name + \".joblib\" if not \".\" in name else name\n","        file = os.path.join(path, name)\n","        joblib.dump(t, file)\n","\n","def get_movs(x, y, n_movs=49, n_features=10):\n","  cor = np.corrcoef(np.nan_to_num(np.concatenate([x, y], axis=1)).T)\n","  corvalid = np.nan_to_num(cor[-49:, :-49])\n","\n","  cttfeat = zip(abs(corvalid).mean(axis=0), range(corvalid.shape[1]))\n","  cttfeat = sorted(cttfeat, reverse=True, key=lambda x:x[0])\n","  cttfeat = np.array([x[1] for x in cttfeat[:n_features]])\n","\n","  cttmovs = zip(abs(corvalid).mean(axis=1), range(49))\n","  cttmovs = sorted(cttmovs, key=lambda x:x[0], reverse=True)\n","  cttmovs = np.array([x[1] for x in cttmovs[:n_movs]])\n","\n","  return cttfeat, cttmovs\n","\n","def save_feats_classes(feats, classes, path, feat_names=None):\n","    if feat_names != None:\n","        with open(os.path.join(path, \"features.txt\"), \"w\") as file:\n","            for feat in np.array(feat_names)[feats]:\n","                file.write(feat+\"\\n\")\n","    with open(os.path.join(path, \"features_number.txt\"), \"w\") as file:\n","        for feat in feats:\n","            file.write(str(feat)+\"\\n\")\n","    with open(os.path.join(path, \"classes.txt\"), \"w\") as file:\n","        for c in classes:\n","            file.write(str(c)+\"\\n\")\n","\n","def get_data(path, subjects=range(1, 41), preprocessing=\"raw\", window_content=\"pure\", window_stride=\"2048_512\"):\n","\n","    features, labels, ids = [], [], []\n","\n","    print(\"Subjects: \")\n","    for subject in subjects:\n","      print(subject, end=\" \")\n","      features.append(np.load(join(path, \"Features_Files\", preprocessing, window_content, window_stride, f\"features_{subject}.npy\")))\n","      labels.append(np.load(join(path, \"Windows_Files\", preprocessing, window_content, window_stride, f\"labels_{subject}.npy\")))\n","      ids.append(np.load(join(path, \"Windows_Files\", preprocessing, window_content, window_stride, f\"ids_{subject}.npy\")))\n","\n","    features = np.concatenate(features)\n","    labels = np.concatenate(labels)\n","    ids = np.concatenate(ids)\n","\n","    return features, labels, ids\n","\n","def get_unique_names(path):\n","\n","    with open(join(path, \"Features_Files\", \"raw\", \"pure\", \"2048_512\", f\"features_names.txt\")) as file:\n","        names = file.read().splitlines()\n","\n","    cols = []\n","    marked = []\n","    names_ = []\n","    for i in range(len(names)):\n","      if not names[i] in marked and not names[i].startswith(\"KL\"):\n","        cols.append(True)\n","        marked.append(names[i])\n","        names_.append(names[i])\n","      else:\n","        cols.append(False)\n","    names = names_\n","    return names, cols\n","\n","def select_feats_and_movs(X, Y, ids,  feats, movs):\n","    ctt = [x in movs for x in Y.argmax(1)]\n","    X_ = X[ctt][:, feats]\n","    Y_ = Y[ctt]\n","    ids_ = ids[ctt]\n","    onehot= OneHotEncoder()\n","    encoder = LabelEncoder()\n","    Y_ = onehot.fit_transform(encoder.fit_transform(Y_.argmax(axis=1)).reshape(-1, 1))\n","    return X_, np.array(Y_.todense()), ids_, onehot, encoder\n","\n","def select_from_folds(*folds, feats=None, movs=None):\n","    res = []\n","    for fold in folds:\n","        res.append(select_feats_and_movs(*fold, feats, movs))\n","    return tuple(res)\n","\n","def make_dirs(*paths):\n","  for path in paths:\n","    try:\n","      os.mkdir(path)\n","    except:\n","      pass\n","\n","def need_to(path):\n","  try:\n","    files = os.listdir(path)\n","  except:\n","    return True\n","  return not \"test_results.txt\" in files"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FkFRv9ixzNhD","colab_type":"code","outputId":"b18ee681-e160-4fa5-8fc5-dfdaac331805","executionInfo":{"status":"ok","timestamp":1570074426007,"user_tz":180,"elapsed":92709,"user":{"displayName":"Paulo Augusto Alves Luz Viana","photoUrl":"","userId":"17456686599891736397"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["subjects = range(1, 41)\n","movs = range(1, 50)\n","\n","features, labels, ids = get_data(DATA_PATH, subjects=subjects)\n","\n","names, cols = get_unique_names(DATA_PATH)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Subjects: \n","1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 "],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RqB6cnjF1HO4","colab_type":"code","outputId":"b788f7d9-7162-46c2-8d78-e4404a0c2048","executionInfo":{"status":"ok","timestamp":1570074705964,"user_tz":180,"elapsed":371735,"user":{"displayName":"Paulo Augusto Alves Luz Viana","photoUrl":"","userId":"17456686599891736397"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"source":["train_data, valid_data, test_data, transformers = create_folds(features, labels, ids, feature_cols=cols, movs=movs)\n","\n","del features, labels, ids"],"execution_count":5,"outputs":[{"output_type":"stream","text":["180571 out of 311579 selected\n","Selecting movements\n","Folding subjects\n","TRAIN # 128152\n","VALID # 22546\n","TEST # 29873\n","Scaling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KbOoyVo-QIvR","colab_type":"code","outputId":"c2e9999c-2f7d-4734-84fa-806b08534aad","executionInfo":{"status":"ok","timestamp":1570082846529,"user_tz":180,"elapsed":4099463,"user":{"displayName":"Paulo Augusto Alves Luz Viana","photoUrl":"","userId":"17456686599891736397"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from sklearn.metrics import confusion_matrix\n","from keras.models import load_model\n","\n","n_classes = [2, 3, 4] + list(range(5, 50, 4))\n","classes = list(range(49))\n","FIT_NN = True\n","FIT_KNN = False\n","FIT_LR = True\n","n_features = len(names)\n","OVERWRITE = False\n","experiments_path = os.path.join(\"/content/drive/My Drive/TCC/Experiments/Final\")\n","\n","nn_folder_name = \"NEURAL_BACKWARD\"\n","lr_folder_name = \"LOGISTIC_BACKWARD\"\n","knn_folder_name = \"KNN_BACKWARD\"\n","\n","make_dirs(os.path.join(experiments_path, nn_folder_name), \n","          os.path.join(experiments_path, lr_folder_name), \n","          os.path.join(experiments_path, knn_folder_name))\n","\n","\n","while len(classes)>=2:\n","  n = len(classes)\n","  print(\"N CLASSES =\", n)\n","  print(classes)\n","  K.clear_session()\n","  \n","  nn_path = os.path.join(experiments_path, nn_folder_name, f\"NCLASSES_{n}\")\n","  lr_path = os.path.join(experiments_path, lr_folder_name, f\"NCLASSES_{n}\")\n","  knn_path = os.path.join(experiments_path, knn_folder_name, f\"NCLASSES_{n}\")\n","\n","  make_dirs(nn_path, lr_path, knn_path)\n","  \n","#   if (not need_to(nn_path) and not need_to(lr_path) and not need_to(knn_path)) and not OVERWRITE:\n","#     continue\n","  \n","  nn_lr = 1e-5\n","  lr_lr = 1e-5\n","  \n","  knn = KNN(n_neighbors=370, mode=\"dist\")\n","  logistic_regression = LogisticRegression(n_features, n, reg_l1=2e-3/n, reg_l2=2e-4/n, lr=lr_lr, save_path=lr_path)\n","  neural_network = Neural_Network(n_features, n, lr=nn_lr, save_path=nn_path)\n","                                           \n","  ctt_features = np.array(range(train_data[0].shape[1]))\n","  ctt_movs = np.array(classes)\n","  \n","  (x_train_, y_train_, ids_train_, onehot_train, encoder_train), \\\n","  (x_valid_, y_valid_, ids_valid_, onehot_valid, encoder_valid), \\\n","  (x_test_, y_test_, ids_test_, onehot_test, encoder_test) = select_from_folds(train_data, valid_data, test_data, feats=ctt_features, movs=ctt_movs)\n","    \n","  for path in (nn_path, lr_path, knn_path):\n","    save_feats_classes(ctt_features, ctt_movs, path, feat_names=names)\n","    save_transformers(transformers, (\"onehot\", \"encoder\", \"scaler1\", \"scaler2\"), path=path)\n","    save_transformers((onehot_train, onehot_valid, onehot_test,\n","                      encoder_train, encoder_valid, encoder_test),\n","                      (\"onehot_train\", \"onehot_valid\", \"onehot_test\",\n","                      \"encoder_train\", \"encoder_valid\", \"encoder_test\"),\n","                      path=path)\n","\n","  \n","  cw = class_weight.compute_class_weight('balanced',\n","                                          np.unique(y_train_.argmax(axis=1)),\n","                                          y_train_.argmax(1))\n","  batch_size = 2**7 if n < 10 else 2**8 if n < 30 else 2**9\n","  #####################################################################################\n","  train_ = (x_train_, y_train_)\n","  valid_ = (x_valid_, y_valid_)\n","  test_ = (x_test_, y_test_)\n","  \n","  def create_metrics(model, path, splits=[\"train\", \"test\", \"valid\"]):\n","    for x, y, ids, fold_name in zip((x_train_, x_valid_, x_test_),\n","                                     (y_train_, y_valid_, y_test_),\n","                                     (ids_train_, ids_valid_, ids_test_),\n","                                     (\"train\", \"valid\", \"test\")):\n","      if fold_name in splits:\n","          pred = model.predict(x)\n","          make_metrics(y, pred, ids=ids,  folder=path, identifier=fold_name+\"_\")\n","\n","  if need_to(lr_path) and FIT_LR or OVERWRITE:\n","    print(\"Fitting Logistic Regression\")\n","    logistic_regression.fit(*train_, epochs=5000, validation_data=valid_, class_weight=cw, batch_size=batch_size, verbose=0, shuffle=True)\n","    create_metrics(logistic_regression, lr_path)\n","    \n","  if need_to(nn_path) and FIT_NN or OVERWRITE:\n","    print(\"Fitting Neural Network\")\n","    neural_network.fit(*train_, epochs=5000, validation_data=valid_, class_weight=cw, batch_size=batch_size, verbose=0, shuffle=True)\n","    create_metrics(neural_network, nn_path)\n","    \n","    \n","  model = load_model(os.path.join(nn_path, \"model.K\"))\n","  pred_valid = model.predict(x_valid_)\n","  cm = confusion_matrix(y_valid_.argmax(1), pred_valid.argmax(1))\n","  accs = [cm[j,j]/cm[j, :].sum() for j in range(len(classes))]\n","  accs = zip(range(len(classes)), accs)\n","  accs = sorted(accs, key=lambda x:x[1], reverse=False)\n","  accs = [x[0] for x in accs]\n","  bad_classes = [encoder_test.inverse_transform([accs[i]])[0] for i in range(4)]\n","  for c in bad_classes:\n","    classes.remove(c)\n"," "],"execution_count":6,"outputs":[{"output_type":"stream","text":["N CLASSES = 49\n","[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3492: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 45\n","[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 41\n","[0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17, 19, 20, 21, 22, 23, 26, 27, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 37\n","[0, 1, 2, 3, 5, 6, 8, 9, 12, 13, 14, 15, 16, 17, 20, 21, 22, 23, 26, 27, 29, 31, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 33\n","[0, 1, 2, 3, 5, 6, 8, 9, 12, 13, 14, 15, 16, 17, 20, 21, 22, 26, 29, 31, 33, 34, 35, 36, 38, 39, 41, 42, 43, 45, 46, 47, 48]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 29\n","[0, 2, 5, 6, 8, 12, 13, 14, 15, 16, 17, 20, 21, 22, 26, 29, 31, 33, 34, 35, 36, 38, 39, 41, 43, 45, 46, 47, 48]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 25\n","[0, 2, 5, 6, 8, 12, 13, 14, 15, 17, 20, 22, 29, 31, 33, 35, 36, 38, 39, 41, 43, 45, 46, 47, 48]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 21\n","[0, 2, 6, 8, 12, 13, 15, 17, 20, 22, 31, 33, 35, 36, 38, 39, 41, 43, 45, 47, 48]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 17\n","[0, 2, 8, 12, 13, 15, 17, 20, 22, 31, 36, 38, 39, 41, 43, 45, 47]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 13\n","[0, 8, 12, 13, 15, 17, 31, 38, 39, 41, 43, 45, 47]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 9\n","[8, 12, 13, 15, 17, 38, 39, 43, 45]\n","Fitting Logistic Regression\n","Fitting Neural Network\n","N CLASSES = 5\n","[8, 17, 38, 43, 45]\n","Fitting Logistic Regression\n","Fitting Neural Network\n"],"name":"stdout"}]}]}